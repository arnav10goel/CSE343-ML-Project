{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 1000)\n",
      "(7500, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Open the .csv files\n",
    "X_train = pd.read_csv('/Users/arnav/Desktop/MachineLearning/ML_CSE343 Project/train_tfidf.csv')\n",
    "X_test = pd.read_csv('/Users/arnav/Desktop/MachineLearning/ML_CSE343 Project/test_tfidf.csv')\n",
    "\n",
    "X_train = X_train.drop(['Unnamed: 0'], axis=1)\n",
    "X_test = X_test.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features into a numpy array\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 1)\n",
      "(7500, 1)\n"
     ]
    }
   ],
   "source": [
    "# Open the labels\n",
    "y_train_df = pd.read_csv('/Users/arnav/Desktop/MachineLearning/ML_CSE343 Project/CSE343-ML-Project/Data/Preprocessed-Data/train_labels.csv')\n",
    "y_test_df = pd.read_csv('/Users/arnav/Desktop/MachineLearning/ML_CSE343 Project/CSE343-ML-Project/Data/Preprocessed-Data/test_labels.csv')\n",
    "\n",
    "# Check the shape of the labels\n",
    "print(y_train_df.shape)\n",
    "print(y_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500,)\n",
      "(7500,)\n"
     ]
    }
   ],
   "source": [
    "# Convert class to non_suicide = 0 and suicide = 1\n",
    "y_train_temp = y_train_df['class'].values\n",
    "y_test_temp = y_test_df['class'].values\n",
    "\n",
    "# Check the shape of the labels\n",
    "print(y_train_temp.shape)\n",
    "print(y_test_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500,)\n",
      "(7500,)\n"
     ]
    }
   ],
   "source": [
    "# Making the Labels Numeric\n",
    "y_train = np.array([0 if label == \"non-suicide\" else 1 for label in y_train_temp])\n",
    "y_test = np.array([0 if label == \"non-suicide\" else 1 for label in y_test_temp])\n",
    "\n",
    "# Check the shape of the labels\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004686103127390068 0.0312074847455884\n"
     ]
    }
   ],
   "source": [
    "# Standard Preprocessing\n",
    "# Check the mean and standard deviation of the data\n",
    "print(np.mean(X_train), np.std(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_base = LogisticRegression()\n",
    "\n",
    "# Fit\n",
    "lr_base.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = lr_base.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Logistic Regression: Test Data\n",
      "Accuracy: 0.9150666666666667\n",
      "Confusion Matrix:\n",
      "[[3429  279]\n",
      " [ 358 3434]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92      3708\n",
      "           1       0.92      0.91      0.92      3792\n",
      "\n",
      "    accuracy                           0.92      7500\n",
      "   macro avg       0.92      0.92      0.92      7500\n",
      "weighted avg       0.92      0.92      0.92      7500\n",
      "\n",
      "AUC Score: 0.9151739994264881\n",
      "Weighted F1-score: 0.9150672630876725\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy\n",
    "print(\"Base Logistic Regression: Test Data\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print AUC score\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Print weighted F1-score\n",
    "print(\"Weighted F1-score:\", f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Logistic Regression: Training Data\n",
      "Accuracy: 0.9210666666666667\n",
      "Confusion Matrix:\n",
      "[[10549   743]\n",
      " [ 1033 10175]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     11292\n",
      "           1       0.93      0.91      0.92     11208\n",
      "\n",
      "    accuracy                           0.92     22500\n",
      "   macro avg       0.92      0.92      0.92     22500\n",
      "weighted avg       0.92      0.92      0.92     22500\n",
      "\n",
      "AUC Score: 0.9210174473068803\n",
      "Weighted F1-score: 0.9210497511596902\n"
     ]
    }
   ],
   "source": [
    "# Predict on training data\n",
    "y_pred_train = lr_base.predict(X_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Base Logistic Regression: Training Data\")\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train, y_pred_train))\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "# Print AUC score\n",
    "print(\"AUC Score:\", roc_auc_score(y_train, y_pred_train))\n",
    "\n",
    "# Print weighted F1-score\n",
    "print(\"Weighted F1-score:\", f1_score(y_train, y_pred_train, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=newton-cg; total time=   0.6s\n",
      "[CV] END ...................C=0.01, penalty=l1, solver=lbfgs; total time=   0.7s\n",
      "[CV] END ...................C=0.01, penalty=l1, solver=lbfgs; total time=   0.6s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=newton-cg; total time=   0.7s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=newton-cg; total time=   0.7s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=newton-cg; total time=   0.7s\n",
      "[CV] END ...................C=0.01, penalty=l1, solver=lbfgs; total time=   0.6s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=newton-cg; total time=   0.7s\n",
      "[CV] END ...................C=0.01, penalty=l1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ...................C=0.01, penalty=l1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   1.1s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   1.1s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   1.1s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   1.1s\n",
      "[CV] END ...............C=0.01, penalty=l1, solver=liblinear; total time=   1.2s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=newton-cg; total time=   7.3s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=newton-cg; total time=   8.0s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=newton-cg; total time=   8.0s\n",
      "[CV] END ...................C=0.01, penalty=l2, solver=lbfgs; total time=   1.0s\n",
      "[CV] END ...................C=0.01, penalty=l2, solver=lbfgs; total time=   1.1s\n",
      "[CV] END ...................C=0.01, penalty=l2, solver=lbfgs; total time=   1.1s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=newton-cg; total time=   4.8s\n",
      "[CV] END ....................C=0.01, penalty=l1, solver=saga; total time=  13.1s\n",
      "[CV] END ...................C=0.01, penalty=l2, solver=lbfgs; total time=   0.9s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=newton-cg; total time=   4.5s\n",
      "[CV] END ....................C=0.01, penalty=l1, solver=saga; total time=  13.7s\n",
      "[CV] END ....................C=0.01, penalty=l1, solver=saga; total time=  13.6s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=liblinear; total time=   0.6s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=liblinear; total time=   0.6s\n",
      "[CV] END ...................C=0.01, penalty=l2, solver=lbfgs; total time=   0.7s\n",
      "[CV] END ....................C=0.01, penalty=l1, solver=saga; total time=  13.1s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=liblinear; total time=   0.6s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=liblinear; total time=   0.6s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ...............C=0.01, penalty=l2, solver=liblinear; total time=   0.8s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ....................C=0.01, penalty=l1, solver=saga; total time=  13.7s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ....................C=0.1, penalty=l1, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.6s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END ................C=0.1, penalty=l1, solver=liblinear; total time=   0.5s\n",
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=   6.0s\n",
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=   6.4s\n",
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=   6.1s\n",
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=   6.7s\n",
      "[CV] END ....................C=0.01, penalty=l2, solver=saga; total time=   6.7s\n",
      "[CV] END .....................C=0.1, penalty=l1, solver=saga; total time=   9.7s\n",
      "[CV] END .....................C=0.1, penalty=l1, solver=saga; total time=   9.4s\n",
      "[CV] END .....................C=0.1, penalty=l1, solver=saga; total time=  10.2s\n",
      "[CV] END ....................C=0.1, penalty=l2, solver=lbfgs; total time=   3.2s\n",
      "[CV] END ....................C=0.1, penalty=l2, solver=lbfgs; total time=   2.3s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=newton-cg; total time=  13.2s\n",
      "[CV] END ....................C=0.1, penalty=l2, solver=lbfgs; total time=   1.8s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=newton-cg; total time=  13.8s\n",
      "[CV] END .....................C=0.1, penalty=l1, solver=saga; total time=  15.5s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=newton-cg; total time=  14.8s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   1.2s\n",
      "[CV] END ....................C=0.1, penalty=l2, solver=lbfgs; total time=   1.8s\n",
      "[CV] END ....................C=0.1, penalty=l2, solver=lbfgs; total time=   2.5s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   1.0s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   1.1s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   1.1s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=liblinear; total time=   1.5s\n",
      "[CV] END .....................C=0.1, penalty=l1, solver=saga; total time=  17.6s\n",
      "[CV] END ..................C=1, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ..................C=1, penalty=l1, solver=newton-cg; total time=   0.3s\n",
      "[CV] END ..................C=1, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ..................C=1, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ..................C=1, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ......................C=1, penalty=l1, solver=lbfgs; total time=   0.3s\n",
      "[CV] END ......................C=1, penalty=l1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=newton-cg; total time=  14.5s\n",
      "[CV] END ......................C=1, penalty=l1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END ......................C=1, penalty=l1, solver=lbfgs; total time=   0.3s\n",
      "[CV] END ......................C=1, penalty=l1, solver=lbfgs; total time=   0.3s\n",
      "[CV] END ................C=0.1, penalty=l2, solver=newton-cg; total time=  14.8s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.8s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.8s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.8s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.6s\n",
      "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.7s\n",
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=   9.2s\n",
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=   9.9s\n",
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=  10.6s\n",
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=   9.8s\n",
      "[CV] END .....................C=0.1, penalty=l2, solver=saga; total time=  10.0s\n",
      "[CV] END ..................C=1, penalty=l2, solver=newton-cg; total time=  12.5s\n",
      "[CV] END ..................C=1, penalty=l2, solver=newton-cg; total time=  12.5s\n",
      "[CV] END ..................C=1, penalty=l2, solver=newton-cg; total time=  12.7s\n",
      "[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   4.0s\n",
      "[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   3.4s\n",
      "[CV] END ..................C=1, penalty=l2, solver=newton-cg; total time=   8.4s\n",
      "[CV] END ..................C=1, penalty=l2, solver=newton-cg; total time=   8.4s\n",
      "[CV] END .......................C=1, penalty=l1, solver=saga; total time=  26.1s\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   0.9s\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   0.8s\n",
      "[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   2.5s\n",
      "[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   2.6s\n",
      "[CV] END ......................C=1, penalty=l2, solver=lbfgs; total time=   2.7s\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   1.0s\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   0.7s\n",
      "[CV] END ..................C=1, penalty=l2, solver=liblinear; total time=   0.7s\n",
      "[CV] END .......................C=1, penalty=l1, solver=saga; total time=  29.6s\n",
      "[CV] END .......................C=1, penalty=l1, solver=saga; total time=  32.6s\n",
      "[CV] END .................C=10, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END .................C=10, penalty=l1, solver=newton-cg; total time=   0.1s\n",
      "[CV] END .......................C=1, penalty=l1, solver=saga; total time=  28.0s\n",
      "[CV] END .................C=10, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END .................C=10, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END .................C=10, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END .....................C=10, penalty=l1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END .....................C=10, penalty=l1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END .....................C=10, penalty=l1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END .....................C=10, penalty=l1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END .....................C=10, penalty=l1, solver=lbfgs; total time=   0.2s\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   0.8s\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   0.8s\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   0.7s\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   0.8s\n",
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time=   6.4s\n",
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time=   5.9s\n",
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time=   6.6s\n",
      "[CV] END .................C=10, penalty=l1, solver=liblinear; total time=   0.9s\n",
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time=   7.0s\n",
      "[CV] END .......................C=1, penalty=l1, solver=saga; total time=  31.8s\n",
      "[CV] END .......................C=1, penalty=l2, solver=saga; total time=   7.7s\n",
      "[CV] END .................C=10, penalty=l2, solver=newton-cg; total time=  16.2s\n",
      "[CV] END .................C=10, penalty=l2, solver=newton-cg; total time=  17.4s\n",
      "[CV] END .................C=10, penalty=l2, solver=newton-cg; total time=  18.2s\n",
      "[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   6.7s\n",
      "[CV] END .................C=10, penalty=l2, solver=newton-cg; total time=  11.4s\n",
      "[CV] END .................C=10, penalty=l2, solver=newton-cg; total time=  11.0s\n",
      "[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   6.1s\n",
      "[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   6.1s\n",
      "[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   5.7s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   1.0s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   0.9s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   0.8s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   0.7s\n",
      "[CV] END .................C=10, penalty=l2, solver=liblinear; total time=   0.7s\n",
      "[CV] END .....................C=10, penalty=l2, solver=lbfgs; total time=   4.2s\n",
      "[CV] END ......................C=10, penalty=l2, solver=saga; total time=  10.1s\n",
      "[CV] END ......................C=10, penalty=l2, solver=saga; total time=  11.6s\n",
      "[CV] END ......................C=10, penalty=l2, solver=saga; total time=  11.7s\n",
      "[CV] END ................C=100, penalty=l1, solver=newton-cg; total time=   0.1s\n",
      "[CV] END ................C=100, penalty=l1, solver=newton-cg; total time=   0.1s\n",
      "[CV] END ................C=100, penalty=l1, solver=newton-cg; total time=   0.1s\n",
      "[CV] END ................C=100, penalty=l1, solver=newton-cg; total time=   0.2s\n",
      "[CV] END ................C=100, penalty=l1, solver=newton-cg; total time=   0.1s\n",
      "[CV] END ....................C=100, penalty=l1, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ....................C=100, penalty=l1, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ....................C=100, penalty=l1, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ....................C=100, penalty=l1, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ....................C=100, penalty=l1, solver=lbfgs; total time=   0.1s\n",
      "[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   0.7s\n",
      "[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   0.7s\n",
      "[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   0.6s\n",
      "[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   0.7s\n",
      "[CV] END ................C=100, penalty=l1, solver=liblinear; total time=   0.7s\n",
      "[CV] END ......................C=10, penalty=l2, solver=saga; total time=  10.9s\n",
      "[CV] END ......................C=10, penalty=l2, solver=saga; total time=  10.3s\n",
      "[CV] END ......................C=10, penalty=l1, solver=saga; total time= 1.3min\n",
      "[CV] END ......................C=10, penalty=l1, solver=saga; total time= 1.4min\n",
      "[CV] END ......................C=10, penalty=l1, solver=saga; total time= 2.1min\n",
      "[CV] END ................C=100, penalty=l2, solver=newton-cg; total time=  10.9s\n",
      "[CV] END ................C=100, penalty=l2, solver=newton-cg; total time=  12.9s\n",
      "[CV] END ................C=100, penalty=l2, solver=newton-cg; total time=  12.9s\n",
      "[CV] END ................C=100, penalty=l2, solver=newton-cg; total time=  13.6s\n",
      "[CV] END ................C=100, penalty=l2, solver=newton-cg; total time=  10.9s\n",
      "[CV] END ......................C=10, penalty=l1, solver=saga; total time= 3.1min\n",
      "[CV] END ....................C=100, penalty=l2, solver=lbfgs; total time=   9.3s\n",
      "[CV] END ....................C=100, penalty=l2, solver=lbfgs; total time=  11.1s\n",
      "[CV] END ....................C=100, penalty=l2, solver=lbfgs; total time=   8.0s\n",
      "[CV] END ....................C=100, penalty=l2, solver=lbfgs; total time=   9.6s\n",
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   1.2s\n",
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   1.2s\n",
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   0.9s\n",
      "[CV] END ....................C=100, penalty=l2, solver=lbfgs; total time=   7.8s\n",
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   1.1s\n",
      "[CV] END ................C=100, penalty=l2, solver=liblinear; total time=   0.9s\n",
      "[CV] END .....................C=100, penalty=l1, solver=saga; total time= 2.8min\n",
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time=  57.7s\n",
      "[CV] END ......................C=10, penalty=l1, solver=saga; total time= 4.7min\n",
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time= 1.2min\n",
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time=  45.1s\n",
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time=  36.0s\n",
      "[CV] END .....................C=100, penalty=l2, solver=saga; total time=  49.1s\n",
      "[CV] END .....................C=100, penalty=l1, solver=saga; total time= 4.4min\n",
      "[CV] END .....................C=100, penalty=l1, solver=saga; total time= 5.5min\n",
      "[CV] END .....................C=100, penalty=l1, solver=saga; total time= 6.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "50 fits failed out of a total of 200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.796      0.79448889 0.8708     0.8708\n",
      " 0.87302222 0.8708            nan        nan 0.88066667 0.88035556\n",
      " 0.89826667 0.89826667 0.8984     0.89826667        nan        nan\n",
      " 0.90924444 0.90924444 0.91008889 0.91008889 0.91008889 0.91008889\n",
      "        nan        nan 0.90742222 0.90742222 0.90866667 0.90866667\n",
      " 0.90866667 0.90866667        nan        nan 0.90537778 0.90533333\n",
      " 0.90555556 0.9056     0.90555556 0.90555556]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....................C=100, penalty=l1, solver=saga; total time= 7.2min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.1, 1, 10, 100], &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;saga&#x27;]},\n",
       "             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.1, 1, 10, 100], &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;saga&#x27;]},\n",
       "             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=1000), n_jobs=-1,\n",
       "             param_grid={'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga']},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Grid Search to find the best parameters for Logistic Regression\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "lr_best = LogisticRegression(max_iter=1000, C=best_parameters['C'], penalty=best_parameters['penalty'], solver=best_parameters['solver'])\n",
    "\n",
    "# Fit the best model    \n",
    "lr_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred_test_best = lr_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression: Test Data\n",
      "Accuracy: 0.9150666666666667\n",
      "Confusion Matrix:\n",
      "[[3429  279]\n",
      " [ 358 3434]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92      3708\n",
      "           1       0.92      0.91      0.92      3792\n",
      "\n",
      "    accuracy                           0.92      7500\n",
      "   macro avg       0.92      0.92      0.92      7500\n",
      "weighted avg       0.92      0.92      0.92      7500\n",
      "\n",
      "AUC Score: 0.9151739994264881\n",
      "Weighted F1-score: 0.9150672630876725\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy\n",
    "print(\"Best Logistic Regression: Test Data\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print AUC score\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Print weighted F1-score\n",
    "print(\"Weighted F1-score:\", f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression: Training Data\n",
      "Accuracy: 0.9211111111111111\n",
      "Confusion Matrix:\n",
      "[[10549   743]\n",
      " [ 1032 10176]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     11292\n",
      "           1       0.93      0.91      0.92     11208\n",
      "\n",
      "    accuracy                           0.92     22500\n",
      "   macro avg       0.92      0.92      0.92     22500\n",
      "weighted avg       0.92      0.92      0.92     22500\n",
      "\n",
      "AUC Score: 0.9210620582990289\n",
      "Weighted F1-score: 0.921094308497212\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "# Predict the labels of the training set\n",
    "y_pred_train_best = lr_best.predict(X_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Best Logistic Regression: Training Data\")\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_pred_train_best))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train, y_pred_train_best))\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_train, y_pred_train_best))\n",
    "\n",
    "# Print AUC score\n",
    "print(\"AUC Score:\", roc_auc_score(y_train, y_pred_train_best))\n",
    "\n",
    "# Print weighted F1-score\n",
    "print(\"Weighted F1-score:\", f1_score(y_train, y_pred_train_best, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot No. of Components vs. Explained Variance and mark at 75% explained variance\n",
    "pca = PCA().fit(X_train)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('No. of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.axhline(y=0.75, color='r', linestyle='-')\n",
    "plt.text(0.5, 0.8, '75%', color = 'red', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Components: 434\n"
     ]
    }
   ],
   "source": [
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components = np.argmax(cumulative_explained_variance >= 0.75) + 1\n",
    "print(\"No. of Components:\", n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the data and extract the features that explain 75% of the variance\n",
    "pca = PCA(n_components=0.75)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 434)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logistic Regression \n",
    "# Initialize Logistic Regression\n",
    "lr_pca = LogisticRegression(max_iter=1000, C=best_parameters['C'], penalty=best_parameters['penalty'], solver=best_parameters['solver'])\n",
    "\n",
    "# Fit the model on the training data\n",
    "lr_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_test_pca = lr_pca.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with PCA: Test Data\n",
      "Accuracy Score: 0.9134666666666666\n",
      "Confusion Matrix:\n",
      " [[3431  277]\n",
      " [ 372 3420]]\n",
      "AUC Score: 0.9135976950281977\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91      3708\n",
      "           1       0.93      0.90      0.91      3792\n",
      "\n",
      "    accuracy                           0.91      7500\n",
      "   macro avg       0.91      0.91      0.91      7500\n",
      "weighted avg       0.91      0.91      0.91      7500\n",
      "\n",
      "Weighted F1-score: 0.9134650590661715\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy score\n",
    "print(\"Logistic Regression with PCA: Test Data\")\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred_test_pca))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test_pca))\n",
    "\n",
    "# Print AUC score\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, y_pred_test_pca))\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test_pca))\n",
    "\n",
    "# Print weighted F1-score\n",
    "print(\"Weighted F1-score:\", f1_score(y_test, y_pred_test_pca, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with PCA: Training Data\n",
      "Accuracy Score: 0.9144\n",
      "Confusion Matrix:\n",
      " [[10468   824]\n",
      " [ 1102 10106]]\n",
      "AUC Score: 0.9143526788592632\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92     11292\n",
      "           1       0.92      0.90      0.91     11208\n",
      "\n",
      "    accuracy                           0.91     22500\n",
      "   macro avg       0.91      0.91      0.91     22500\n",
      "weighted avg       0.91      0.91      0.91     22500\n",
      "\n",
      "Weighted F1-score: 0.9143829794132429\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "# Predict on the training data\n",
    "y_pred_train_pca = lr_pca.predict(X_train_pca)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Logistic Regression with PCA: Training Data\")\n",
    "print(\"Accuracy Score:\", accuracy_score(y_train, y_pred_train_pca))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_train, y_pred_train_pca))\n",
    "\n",
    "# Print AUC score\n",
    "print(\"AUC Score:\", roc_auc_score(y_train, y_pred_train_pca))\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, y_pred_train_pca))\n",
    "\n",
    "# Print weighted F1-score\n",
    "print(\"Weighted F1-score:\", f1_score(y_train, y_pred_train_pca, average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
